### [CVE-2025-53630](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2025-53630)
![](https://img.shields.io/static/v1?label=Product&message=llama.cpp&color=blue)
![](https://img.shields.io/static/v1?label=Version&message=%3C%2026a48ad699d50b6268900062661bd22f3e792579%20&color=brightgreen)
![](https://img.shields.io/static/v1?label=Vulnerability&message=CWE-122%3A%20Heap-based%20Buffer%20Overflow&color=brightgreen)
![](https://img.shields.io/static/v1?label=Vulnerability&message=CWE-680%3A%20Integer%20Overflow%20to%20Buffer%20Overflow&color=brightgreen)

### Description

llama.cpp is an inference of several LLM models in C/C++. Integer Overflow in the gguf_init_from_file_impl function in ggml/src/gguf.cpp can lead to Heap Out-of-Bounds Read/Write. This vulnerability is fixed in commit 26a48ad699d50b6268900062661bd22f3e792579.

### POC

#### Reference
- https://github.com/ggml-org/llama.cpp/security/advisories/GHSA-vgg9-87g3-85w8

#### Github
No PoCs found on GitHub currently.

