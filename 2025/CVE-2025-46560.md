### [CVE-2025-46560](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2025-46560)
![](https://img.shields.io/static/v1?label=Product&message=vllm&color=blue)
![](https://img.shields.io/static/v1?label=Version&message=%3E%3D%200.8.0%2C%20%3C%200.8.5%20&color=brightgreen)
![](https://img.shields.io/static/v1?label=Vulnerability&message=CWE-1333%3A%20Inefficient%20Regular%20Expression%20Complexity&color=brightgreen)

### Description

vLLM is a high-throughput and memory-efficient inference and serving engine for LLMs. Versions starting from 0.8.0 and prior to 0.8.5 are affected by a critical performance vulnerability in the input preprocessing logic of the multimodal tokenizer. The code dynamically replaces placeholder tokens (e.g., <|audio_|>, <|image_|>) with repeated tokens based on precomputed lengths. Due to ​​inefficient list concatenation operations​​, the algorithm exhibits ​​quadratic time complexity (O(n²))​​, allowing malicious actors to trigger resource exhaustion via specially crafted inputs. This issue has been patched in version 0.8.5.

### POC

#### Reference
- https://github.com/vllm-project/vllm/security/advisories/GHSA-vc6m-hm49-g9qg

#### Github
- https://github.com/fkie-cad/nvd-json-data-feeds

