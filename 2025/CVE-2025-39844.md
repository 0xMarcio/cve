### [CVE-2025-39844](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2025-39844)
![](https://img.shields.io/static/v1?label=Product&message=Linux&color=blue)
![](https://img.shields.io/static/v1?label=Version&message=&color=brightgreen)
![](https://img.shields.io/static/v1?label=Version&message=5.13%20&color=brightgreen)
![](https://img.shields.io/static/v1?label=Version&message=8d400913c231bd1da74067255816453f96cd35b0%20&color=brightgreen)
![](https://img.shields.io/static/v1?label=Vulnerability&message=n%2Fa&color=blue)

### Description

In the Linux kernel, the following vulnerability has been resolved:mm: move page table sync declarations to linux/pgtable.hDuring our internal testing, we started observing intermittent bootfailures when the machine uses 4-level paging and has a large amount ofpersistent memory:  BUG: unable to handle page fault for address: ffffe70000000034  #PF: supervisor write access in kernel mode  #PF: error_code(0x0002) - not-present page  PGD 0 P4D 0   Oops: 0002 [#1] SMP NOPTI  RIP: 0010:__init_single_page+0x9/0x6d  Call Trace:   <TASK>   __init_zone_device_page+0x17/0x5d   memmap_init_zone_device+0x154/0x1bb   pagemap_range+0x2e0/0x40f   memremap_pages+0x10b/0x2f0   devm_memremap_pages+0x1e/0x60   dev_dax_probe+0xce/0x2ec [device_dax]   dax_bus_probe+0x6d/0xc9   [... snip ...]   </TASK>It turns out that the kernel panics while initializing vmemmap (structpage array) when the vmemmap region spans two PGD entries, because the newPGD entry is only installed in init_mm.pgd, but not in the page tables ofother tasks.And looking at __populate_section_memmap():  if (vmemmap_can_optimize(altmap, pgmap))                                          // does not sync top level page tables          r = vmemmap_populate_compound_pages(pfn, start, end, nid, pgmap);  else                                                                              // sync top level page tables in x86          r = vmemmap_populate(start, end, nid, altmap);In the normal path, vmemmap_populate() in arch/x86/mm/init_64.csynchronizes the top level page table (See commit 9b861528a801 ("x86-64,mem: Update all PGDs for direct mapping and vmemmap mapping changes")) sothat all tasks in the system can see the new vmemmap area.However, when vmemmap_can_optimize() returns true, the optimized pathskips synchronization of top-level page tables.  This is becausevmemmap_populate_compound_pages() is implemented in core MM code, whichdoes not handle synchronization of the top-level page tables.  Instead,the core MM has historically relied on each architecture to perform thissynchronization manually.We're not the first party to encounter a crash caused by not-sync'd toplevel page tables: earlier this year, Gwan-gyeong Mun attempted to addressthe issue [1] [2] after hitting a kernel panic when x86 code accessed thevmemmap area before the corresponding top-level entries were synced.  Atthat time, the issue was believed to be triggered only when struct pagewas enlarged for debugging purposes, and the patch did not get furtherupdates.It turns out that current approach of relying on each arch to handle thepage table sync manually is fragile because 1) it's easy to forget to syncthe top level page table, and 2) it's also easy to overlook that thekernel should not access the vmemmap and direct mapping areas before thesync.# The solution: Make page table sync more code robust and harder to missTo address this, Dave Hansen suggested [3] [4] introducing{pgd,p4d}_populate_kernel() for updating kernel portion of the page tablesand allow each architecture to explicitly perform synchronization wheninstalling top-level entries.  With this approach, we no longer need toworry about missing the sync step, reducing the risk of futureregressions.The new interface reuses existing ARCH_PAGE_TABLE_SYNC_MASK,PGTBL_P*D_MODIFIED and arch_sync_kernel_mappings() facility used byvmalloc and ioremap to synchronize page tables.pgd_populate_kernel() looks like this:static inline void pgd_populate_kernel(unsigned long addr, pgd_t *pgd,                                       p4d_t *p4d){        pgd_populate(&init_mm, pgd, p4d);        if (ARCH_PAGE_TABLE_SYNC_MASK & PGTBL_PGD_MODIFIED)                arch_sync_kernel_mappings(addr, addr);}It is worth noting that vmalloc() and apply_to_range() carefullysynchronizes page tables by calling p*d_alloc_track() andarch_sync_kernel_mappings(), and thus they are not affected by---truncated---

### POC

#### Reference
No PoCs from references.

#### Github
- https://github.com/w4zu/Debian_security

