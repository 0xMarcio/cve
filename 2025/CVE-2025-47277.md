### [CVE-2025-47277](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2025-47277)
![](https://img.shields.io/static/v1?label=Product&message=vllm&color=blue)
![](https://img.shields.io/static/v1?label=Version&message=%3E%3D%200.6.5%2C%20%3C%200.8.5%20&color=brightgreen)
![](https://img.shields.io/static/v1?label=Vulnerability&message=CWE-502%3A%20Deserialization%20of%20Untrusted%20Data&color=brightgreen)

### Description

vLLM, an inference and serving engine for large language models (LLMs), has an issue in versions 0.6.5 through 0.8.4 that ONLY impacts environments using the `PyNcclPipe` KV cache transfer integration with the V0 engine. No other configurations are affected. vLLM supports the use of the `PyNcclPipe` class to establish a peer-to-peer communication domain for data transmission between distributed nodes. The GPU-side KV-Cache transmission is implemented through the `PyNcclCommunicator` class, while CPU-side control message passing is handled via the `send_obj` and `recv_obj` methods on the CPU side.​ The intention was that this interface should only be exposed to a private network using the IP address specified by the `--kv-ip` CLI parameter. The vLLM documentation covers how this must be limited to a secured network. The default and intentional behavior from PyTorch is that the `TCPStore` interface listens on ALL interfaces, regardless of what IP address is provided. The IP address given was only used as a client-side address to use. vLLM was fixed to use a workaround to force the `TCPStore` instance to bind its socket to a specified private interface. As of version 0.8.5, vLLM limits the `TCPStore` socket to the private interface as configured.

### POC

#### Reference
- https://docs.vllm.ai/en/latest/deployment/security.html

#### Github
- https://github.com/ARPSyndicate/cve-scores
- https://github.com/Threekiii/CVE
- https://github.com/honysyang/eleaipoc
- https://github.com/tanjiti/sec_profile

