### [CVE-2025-52566](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2025-52566)
![](https://img.shields.io/static/v1?label=Product&message=llama.cpp&color=blue)
![](https://img.shields.io/static/v1?label=Version&message=%3C%20b5721%20&color=brightgreen)
![](https://img.shields.io/static/v1?label=Vulnerability&message=CWE-119%3A%20Improper%20Restriction%20of%20Operations%20within%20the%20Bounds%20of%20a%20Memory%20Buffer&color=brightgreen)
![](https://img.shields.io/static/v1?label=Vulnerability&message=CWE-195%3A%20Signed%20to%20Unsigned%20Conversion%20Error&color=brightgreen)

### Description

llama.cpp is an inference of several LLM models in C/C++. Prior to version b5721, there is a signed vs. unsigned integer overflow in llama.cpp's tokenizer implementation (llama_vocab::tokenize) (src/llama-vocab.cpp:3036) resulting in unintended behavior in tokens copying size comparison. Allowing heap-overflowing llama.cpp inferencing engine with carefully manipulated text input during tokenization process. This issue has been patched in version b5721.

### POC

#### Reference
- https://github.com/ggml-org/llama.cpp/security/advisories/GHSA-7rxv-5jhh-j6xx

#### Github
- https://github.com/fkie-cad/nvd-json-data-feeds

